{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYtLiV39Gt_i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qaxUVwX0RXA"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras.utils import to_categorical\n",
        "import xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8PCszLcU2shb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm  # Install tqdm if not already installed: pip install tqdm\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Set directories\n",
        "Train_DIR = r\"/content/drive/MyDrive/cpmbined data train\"\n",
        "TEST_DIR = r\"/content/drive/MyDrive/final train and test/test\"\n",
        "result_excel_link = r\"/content/drive/MyDrive/ResNet_epoch25.xlsx\"\n",
        "save_dir = r\"/content/drive/MyDrive/DTS/models\"\n",
        "model_name = 'Alexnet_ecg_model_25epochs.keras'\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = 224\n",
        "num_classes = 4\n",
        "batch_size = 16\n",
        "epochs = 25\n",
        "\n",
        "test_img_num = 50  # 100 per class\n",
        "classes = ['Normal Heartbeat', 'Abnormal Heartbeat', 'Myocardial Infarction (MI)', 'History of MI']\n",
        "class_map = {label: i for i, label in enumerate(classes)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UleveD3fEL1z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to load data from folder structure with loading feedback\n",
        "def load_data_from_directory(base_dir, classes, class_map, IMG_SIZE):\n",
        "    data = []\n",
        "    total_images = sum(len(os.listdir(os.path.join(base_dir, label))) for label in classes)\n",
        "\n",
        "    print(f\"Loading {total_images} images from '{base_dir}'...\")\n",
        "\n",
        "    # Use tqdm to create a progress bar\n",
        "    with tqdm(total=total_images, unit=\"image\") as pbar:\n",
        "        for label in classes:\n",
        "            path = os.path.join(base_dir, label)\n",
        "            for img_name in os.listdir(path):\n",
        "                try:\n",
        "                    img_path = os.path.join(path, img_name)\n",
        "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                    # Check if image is successfully loaded\n",
        "                    if img is None:\n",
        "                        raise ValueError(f\"Image {img_name} could not be read.\")\n",
        "\n",
        "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
        "\n",
        "                    # Append image, label, and name to data\n",
        "                    data.append([np.array(img), class_map[label], img_name])\n",
        "\n",
        "                    # Update progress bar\n",
        "                    pbar.update(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {img_name}: {e}\")\n",
        "                    pbar.update(1)  # Still update progress even if there's an error\n",
        "\n",
        "    shuffle(data)\n",
        "    print(\"Loading complete!\")\n",
        "    return data\n",
        "\n",
        "# Load train and test data\n",
        "train_data = load_data_from_directory(Train_DIR, classes, class_map, IMG_SIZE)\n",
        "test_data = load_data_from_directory(TEST_DIR, classes, class_map, IMG_SIZE)\n",
        "\n",
        "# Prepare training and testing datasets\n",
        "trainImages = np.array([i[0] for i in train_data]).astype('float32') / 255.0  # Normalize pixel values\n",
        "trainLabels = np.array([i[1] for i in train_data])  # Numeric labels\n",
        "trainLabels = to_categorical(trainLabels, num_classes)  # Convert labels to one-hot encoding\n",
        "\n",
        "testImages = np.array([i[0] for i in test_data]).astype('float32') / 255.0  # Normalize pixel values\n",
        "testLabels = np.array([i[1] for i in test_data])  # Numeric labels\n",
        "testLabels = to_categorical(testLabels, num_classes)  # Convert labels to one-hot encoding\n",
        "\n",
        "print(\"Data preparation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iR0RCG_OOpp"
      },
      "outputs": [],
      "source": [
        "print(trainImages.shape, trainLabels.shape)\n",
        "print(np.unique(np.argmax(trainLabels, axis=1)))  # Ensure label range is correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRuHiSQdKb6s"
      },
      "outputs": [],
      "source": [
        "trainImages = trainImages.astype('float32') / 255.0\n",
        "testImages = testImages.astype('float32') / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iL6Byylo1ISc"
      },
      "outputs": [],
      "source": [
        "# Learning rate scheduler\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch > 15:\n",
        "        return lr * 0.1\n",
        "    elif epoch > 10:\n",
        "        return lr * 0.5\n",
        "    return lr\n",
        "\n",
        "\n",
        "# Build AlexNet\n",
        "model = Sequential()\n",
        "\n",
        "# CONV Block 1\n",
        "model.add(Conv2D(96, (11,11), strides=(4,4), input_shape=(224,224,3), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV Block 2\n",
        "model.add(Conv2D(256, (11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV Block 3\n",
        "model.add(Conv2D(384, (3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV Block 4\n",
        "model.add(Conv2D(384, (3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV Block 5\n",
        "model.add(Conv2D(256, (3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Fully Connected Block\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "if not os.path.isdir(save_dir): os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "es = EarlyStopping(monitor='val_loss', patience=200, verbose=1)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, es]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    trainImages, trainLabels, test_size=0.2,\n",
        "    stratify=np.argmax(trainLabels, axis=1),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Load best model\n",
        "best_model = load_model(filepath)\n",
        "\n",
        "# Predict\n",
        "preds = best_model.predict(testImages)\n",
        "\n",
        "# Save results to Excel\n",
        "workbook = xlsxwriter.Workbook(result_excel_link)\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write(0, 0, \"Filename\")\n",
        "worksheet.write(0, 1, \"Original Label\")\n",
        "worksheet.write(0, 2, \"Predicted Label\")\n",
        "\n",
        "for i, (img_arr, true_label, filename) in enumerate(test_data[:test_img_num]):\n",
        "    pred_label = np.argmax(preds[i])\n",
        "    worksheet.write(i + 1, 0, filename)\n",
        "    worksheet.write(i + 1, 1, true_label)\n",
        "    worksheet.write(i + 1, 2, pred_label)\n",
        "\n",
        "workbook.close()\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u6rSt-kT2B_L"
      },
      "outputs": [],
      "source": [
        "# Block 9: Evaluate the Model and Generate Metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the best saved model\n",
        "try:\n",
        "    best_model = load_model(f'{model_type}_best.h5')\n",
        "    print(f\"Loaded best model from {model_type}_best.h5\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading best model: {e}. Using the current model instead.\")\n",
        "    best_model = model  # Fallback to the trained model if checkpoint loading fails\n",
        "\n",
        "# Evaluate the model on test data\n",
        "scores = best_model.evaluate(testImages, testLabels, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# Predict on test data\n",
        "preds = best_model.predict(testImages)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "# Convert one-hot encoded testLabels to class indices\n",
        "true_labels = np.argmax(testLabels, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "class_names = classes  # Use the defined classes ['N', 'L', 'R', 'A', 'V']\n",
        "report = classification_report(true_labels, pred_labels, target_names=class_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Save results to Excel\n",
        "try:\n",
        "    workbook = xlsxwriter.Workbook(result_excel_link)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "    # Write headers\n",
        "    worksheet.write(0, 0, \"Filename\")\n",
        "    worksheet.write(0, 1, \"Original Label\")\n",
        "    worksheet.write(0, 2, \"Predicted Label\")\n",
        "    worksheet.write(0, 3, \"Match (1/0)\")\n",
        "    worksheet.write(0, 4, \"Total Execution Time\")\n",
        "\n",
        "    # Write data\n",
        "    test_img_num = len(testImages)\n",
        "    for i, (img_arr, true_label_numeric, filename) in enumerate(test_data[:test_img_num]):\n",
        "        pred_label_numeric = np.argmax(preds[i])\n",
        "        true_label = classes[true_label_numeric]\n",
        "        pred_label = classes[pred_label_numeric]\n",
        "        match = 1 if true_label == pred_label else 0\n",
        "        worksheet.write(i +1, 0, filename)\n",
        "        worksheet.write(i + 1, 1, true_label)\n",
        "        worksheet.write(i + 1, 2, pred_label)\n",
        "        worksheet.write(i + 1, 3, match)\n",
        "\n",
        "    # Write total execution time\n",
        "    end1 = time.time()\n",
        "    time_taken = end1 - start1\n",
        "    worksheet.write(1, 4, time_taken)\n",
        "\n",
        "    # Close the workbook\n",
        "    workbook.close()\n",
        "    print(f\"Results successfully written to {result_excel_link}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while writing to Excel: {e}\")\n",
        "\n",
        "# Plot training results\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yv3Anpto9nPc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}